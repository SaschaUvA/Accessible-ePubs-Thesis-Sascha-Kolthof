{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import cssutils\n",
    "import logging\n",
    "cssutils.log.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ebook and run Ace Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kindly provide the filename of the EPUB: e9a21834-1853-40ae-a2ea-0515d2ade7a3\n"
     ]
    }
   ],
   "source": [
    "# Get Path\n",
    "filename = input(\"Kindly provide the filename of the EPUB: \")\n",
    "filepath = filename + '.epub'\n",
    "\n",
    "# Run Ace Checker on Epub\n",
    "# Uncomment for first use to navigate to sub-folder\n",
    "# %cd OAPEN Ebooks\n",
    "# !ace --verbose --force --subdir --outdir results $filepath\n",
    "# !ace --verbose --subdir --outdir results $filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.63 ms\n"
     ]
    }
   ],
   "source": [
    "# Open EPUB Ace results folder\n",
    "with open('results/'+filename+'/report.json', encoding=\"utf8\") as f:\n",
    "    \n",
    "    # Load report\n",
    "    report = json.load(f)\n",
    "\n",
    "    # Capture metadata\n",
    "    metadata = report['earl:testSubject']['metadata']\n",
    "\n",
    "    # Find assertions\n",
    "    assertions=report['assertions']\n",
    "    \n",
    "    # Create list with errors\n",
    "    errors=[]\n",
    "    for entry in assertions:\n",
    "\n",
    "        # If any errors are found\n",
    "        if entry['earl:result']['earl:outcome'] == 'fail':\n",
    "\n",
    "            # Look over all assertions\n",
    "            for assertion in entry['assertions']:\n",
    "\n",
    "                # If an error is found\n",
    "                if assertion['earl:result']['earl:outcome']=='fail':\n",
    "\n",
    "                    # Add error to list of errors\n",
    "                    errors.append(assertion['earl:test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract error titles and check for target errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "PageBreakFix = False\n",
    "LinkFix = False\n",
    "\n",
    "error_titles = {error['dct:title'] for error in errors}\n",
    "\n",
    "# Set value to true if pagebreak fix needs to be applied\n",
    "if 'epub-pagelist-missing-pagebreak' in error_titles or 'epub-pagelist-broken' in error_titles:\n",
    "    PageBreakFix = True\n",
    "    print('PageBreakFix: True')\n",
    "\n",
    "# Set value to true if link in text fix needs to be applied\n",
    "if 'link-in-text-block' in error_titles:\n",
    "    LinkFix = True\n",
    "    print('LinkFix: True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and dissassemble with Ebooklib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.1 ms\n"
     ]
    }
   ],
   "source": [
    "if PageBreakFix or LinkFix:\n",
    "    # Ignore NCX is turned on by default to avoid duplication at end of pipeline\n",
    "    book = epub.read_epub(filepath)\n",
    "#     book = epub.read_epub(filepath, options={'ignore_ncx ': True})\n",
    "else:\n",
    "    print(\"No errors detected (within this thesis' scope)\")\n",
    "#     exit()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadHTMLSnippets(error_type):\n",
    "    \"\"\"\n",
    "    Finds HTML snippets of associated error type per EPUB item.\n",
    "    \"\"\"\n",
    "\n",
    "    linkfix_dict = {}\n",
    "\n",
    "    for assertion in report['assertions']:\n",
    "        chapter = assertion['earl:testSubject']['url']\n",
    "        linkfix_dict[chapter] = []\n",
    "\n",
    "        # Find HTML snippet of link-in-text-block errors\n",
    "        for a in assertion['assertions']:\n",
    "            if a['earl:test']['dct:title'] == error_type:\n",
    "\n",
    "                # Add HTML snippet for finding this error with BS4\n",
    "                linkfix_dict[chapter].append(a['earl:result']['html'])\n",
    "    \n",
    "    return linkfix_dict\n",
    "\n",
    "\n",
    "\n",
    "def RefineHTMLSnippets(linkfix_dict):\n",
    "    \"\"\"\n",
    "    Refines the HTML snippets so they are easier to match in the BS4 soup\n",
    "    \"\"\"\n",
    "    \n",
    "    refined_dict = {}\n",
    "    href_pattern = r'href=\".*?\"'\n",
    "    \n",
    "    for chapter in linkfix_dict:\n",
    "        refined_dict[chapter] = []\n",
    "        \n",
    "        for snippet in linkfix_dict[chapter]:\n",
    "            re_match_object = re.search(href_pattern, snippet)\n",
    "            refined_dict[chapter].append(re_match_object[0])\n",
    "            \n",
    "    return refined_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.8 ms\n"
     ]
    }
   ],
   "source": [
    "if LinkFix:\n",
    "    \n",
    "    # Load refined HTML snippets of link-in-text-block errors\n",
    "    linkfix_dict = LoadHTMLSnippets('link-in-text-block')\n",
    "    href_dict = RefineHTMLSnippets(linkfix_dict)\n",
    "    \n",
    "    # Keep track of which chapters are altered for EPUB reassemble\n",
    "    chapter_item_list = []\n",
    "    href_pattern = r'href=\".*?\"'\n",
    "    error_links = []\n",
    "    unerror_links = []\n",
    "    \n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "\n",
    "        # Skip item if doesn't appear in dictionary\n",
    "        if item.get_name() not in href_dict:\n",
    "            chapter_item_list.append(item)\n",
    "            continue\n",
    "            \n",
    "        # Skip item if no HTML snippets for this chapter\n",
    "        if not href_dict[item.get_name()]:\n",
    "            chapter_item_list.append(item)\n",
    "            continue\n",
    "        \n",
    "        # Load HTML snippets for current chapter\n",
    "        html_snippets = href_dict[item.get_name()]\n",
    "\n",
    "        # Create BS4 object from chapter\n",
    "        soup = BeautifulSoup(item.content, 'html.parser')\n",
    "\n",
    "        # Find all links\n",
    "        links = soup.find_all('a')\n",
    "\n",
    "        for link in links:\n",
    "            \n",
    "            # skip link if it has no href\n",
    "            if 'href' not in link.attrs:\n",
    "                continue\n",
    "\n",
    "            link_href = re.search(href_pattern, str(link))[0]\n",
    "            \n",
    "            if link_href in html_snippets:\n",
    "                link['style'] = \"text-decoration:underline\"\n",
    "                unerror_links.append((item.get_name(), link_href, html_snippets))\n",
    "            else:\n",
    "                error_links.append((item.get_name(), link_href, html_snippets))\n",
    "\n",
    "\n",
    "        item.content = bytes(str(soup), 'utf-8')\n",
    "        chapter_item_list.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save modified EPUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 760 ms\n"
     ]
    }
   ],
   "source": [
    "# Reassemble the EPUB\n",
    "updated_book = epub.EpubBook()\n",
    "updated_book.set_title(book.get_metadata('DC', 'title')[0][0])\n",
    "updated_book.set_language(book.get_metadata('DC', 'language')[0][0])\n",
    "\n",
    "updated_book.set_identifier(book.get_metadata('DC', 'identifier')[0][0])\n",
    "updated_book.add_metadata('DC', 'creator', book.get_metadata('DC', 'creator')[0][0])\n",
    "try:\n",
    "    updated_book.add_metadata('DC', 'publisher', book.get_metadata('DC', 'publisher')[0][0])\n",
    "except IndexError:\n",
    "    updated_book.add_metadata('DC', 'publisher', 'unknown publisher')\n",
    "    print('unknown publisher here')\n",
    "\n",
    "for item in book.get_items():\n",
    "    # 9 = ITEM_DOCUMENT \n",
    "    if item.get_type() == 9:\n",
    "#         print('found content, break')\n",
    "        break\n",
    "    updated_book.add_item(item)\n",
    "#     print('added item', item)\n",
    "\n",
    "# print('\\nchapter added here\\n')    \n",
    "for item in chapter_item_list:\n",
    "    updated_book.add_item(item)\n",
    "#     print('added a chapter', item)\n",
    "\n",
    "checksum = []\n",
    "for item in book.get_items():\n",
    "    if checksum:\n",
    "        if item.get_type() != 9:\n",
    "            updated_book.add_item(item)\n",
    "    elif item.get_type() == 9:\n",
    "        checksum.append('checkvalue')\n",
    "        \n",
    "# Take the original book's table of content and spine\n",
    "updated_book.toc = book.toc\n",
    "updated_book.spine = book.spine\n",
    "\n",
    "# Add the Ncx and Nav file\n",
    "updated_book.add_item(epub.EpubNcx())\n",
    "updated_book.add_item(epub.EpubNav())\n",
    "\n",
    "# Save the Updated EPUB\n",
    "epub.write_epub('pre_patch.zip', updated_book, {})\n",
    "epub.write_epub('pre_patch.epub', updated_book, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Ebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "head_dict = {}\n",
    "for item in book.get_items():\n",
    "    if item.get_name().endswith('.xhtml') or item.get_name().endswith('.html') or item.get_name().endswith('.htm'):\n",
    "        soup = BeautifulSoup(item.content, 'html.parser')\n",
    "        head_tag = soup.find('head')\n",
    "        head_dict[item.get_name().split('/')[-1]] = head_tag\n",
    "        \n",
    "def is_text_decoration_none_important(decl):\n",
    "    return (decl.name == 'text-decoration' and decl.value == 'none' and decl.priority == 'important'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "input_archive = zipfile.ZipFile(\"pre_patch.epub\", \"r\")\n",
    "output_archive_epub = zipfile.ZipFile(\"post_patch.epub\", \"w\")\n",
    "output_archive_zip = zipfile.ZipFile(\"post_patch.zip\", \"w\")\n",
    "old_book = zipfile.ZipFile(filepath, \"r\")\n",
    "\n",
    "old_list = old_book.infolist()\n",
    "file_list = input_archive.infolist()\n",
    "\n",
    "for x in range(0, len(old_list)):\n",
    "    if old_list[x].filename.endswith(\".opf\"):\n",
    "        \n",
    "        item = old_book.open(old_list[x])\n",
    "        content = item.read()\n",
    "        soup = BeautifulSoup(content, 'lxml-xml')\n",
    "        metadata = soup.find('metadata')\n",
    "        \n",
    "        schema_sufficient = ''\n",
    "        schema_summary = ''\n",
    "        schema_mode = ''\n",
    "        schema_feature = ''\n",
    "        schema_hazard = ''        \n",
    "        \n",
    "        for meta in metadata.find_all('meta'):\n",
    "            if 'property' not in meta.attrs:\n",
    "                continue\n",
    "            if str(meta['property']) == \"schema:accessModeSufficient\":\n",
    "                schema_sufficient = meta.string\n",
    "            elif str(meta['property']) == \"schema:accessibilitySummary\":\n",
    "                schema_summary = meta.string\n",
    "            elif str(meta['property']) == \"schema:accessMode\":\n",
    "                schema_mode = meta.string\n",
    "            elif str(meta['property']) == \"schema:accessibilityFeature\":\n",
    "                schema_feature = meta.string\n",
    "            elif str(meta['property']) == \"schema:accessibilityHazard\":\n",
    "                schema_hazard = meta.string\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "for x in range(0, len(file_list)):\n",
    "    item = input_archive.open(file_list[x])\n",
    "    content = item.read()\n",
    "    \n",
    "    if file_list[x].filename.endswith(\".xhtml\") or file_list[x].filename.endswith(\".html\") or file_list[x].filename.endswith(\".htm\"):\n",
    "\n",
    "        # Find head tag inside content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        head_tag = soup.find('head')\n",
    "        \n",
    "        # Try to replace the empty head content with the old content or replace with default\n",
    "        try:\n",
    "            head_tag.replace_with(head_dict[file_list[x].filename.split('/')[-1]])\n",
    "        except KeyError:\n",
    "            head_tag.replace_with(head_dict[list(head_dict.keys())[0]])\n",
    "        \n",
    "        modification = bytes(str(soup), 'utf-8')\n",
    "        \n",
    "        output_archive_epub.writestr(file_list[x].filename, modification)\n",
    "        output_archive_zip.writestr(file_list[x].filename, modification)\n",
    "    \n",
    "    \n",
    "    elif file_list[x].filename.endswith(\".opf\"):\n",
    "        \n",
    "        # Load soup content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        metadata = soup.find('metadata')\n",
    "        package = soup.find('package')\n",
    "        \n",
    "        if PageBreakFix:\n",
    "        # Add metadata to the .opf file that describes the origin of the pagefile\n",
    "            pagesource_tag = soup.new_tag('dc:source', id=\"pg-src\")\n",
    "            meta_tag_1 = soup.new_tag('meta', property=\"source-of\", refines=\"#pg-src\")\n",
    "            meta_tag_2 = soup.new_tag('meta', property=\"pageBreakSource\")\n",
    "\n",
    "            pagesource_tag.string = 'AccessiPub'\n",
    "            meta_tag_1.string = 'pagination'\n",
    "            meta_tag_2.string = 'AccessiPub'\n",
    "\n",
    "            metadata.append(pagesource_tag)\n",
    "            metadata.append(meta_tag_1)\n",
    "            metadata.append(meta_tag_2)\n",
    "        \n",
    "        \n",
    "        if schema_sufficient:\n",
    "            schema_sufficient_tag = soup.new_tag('meta', property=\"schema:accessModeSufficient\")\n",
    "            schema_sufficient_tag.string = schema_sufficient\n",
    "            metadata.append(schema_sufficient_tag)\n",
    "        \n",
    "        \n",
    "        if schema_summary:\n",
    "            schema_summary_tag = soup.new_tag('meta', property=\"schema:accessibilitySummary\")\n",
    "            schema_summary_tag.string = schema_summary\n",
    "            metadata.append(schema_summary_tag)\n",
    "            \n",
    "            \n",
    "        if schema_mode:\n",
    "            schema_mode_tag = soup.new_tag('meta', property=\"schema:accessMode\")\n",
    "            schema_mode_tag.string = schema_mode\n",
    "            metadata.append(schema_mode_tag)\n",
    "            \n",
    "            \n",
    "        if schema_feature:\n",
    "            schema_feature_tag = soup.new_tag('meta', property=\"schema:accessibilityFeature\")\n",
    "            schema_feature_tag.string = schema_feature\n",
    "            metadata.append(schema_feature_tag)\n",
    "            \n",
    "            \n",
    "        if schema_hazard:\n",
    "            schema_hazard_tag = soup.new_tag('meta', property=\"schema:accessibilityHazard\")\n",
    "            schema_hazard_tag.string = schema_hazard\n",
    "            metadata.append(schema_hazard_tag)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Take language from DC metadata and add it to <package> tag for higher accessibility        \n",
    "        # Only add language if not defined yet\n",
    "        if package.find('xml:lang') is None:\n",
    "            try:\n",
    "                package['xml:lang'] = book.get_metadata('DC', 'language')[0][0]\n",
    "            except IndexError:\n",
    "                package['xml:lang'] = \"Language was not specified in metadata\"\n",
    "                print('unknown language here')\n",
    "        \n",
    "        \n",
    "        # Load modification back into bytes and write the file\n",
    "        modification = bytes(str(soup), 'utf-8')\n",
    "        \n",
    "        output_archive_epub.writestr(file_list[x].filename, modification)\n",
    "        output_archive_zip.writestr(file_list[x].filename, modification)\n",
    "    \n",
    "    elif file_list[x].filename.endswith(\".css\"):\n",
    "        \n",
    "        # Parse content string into css sheet\n",
    "        try:\n",
    "            sheet = cssutils.parseString(content)\n",
    "        except UnicodeDecodeError:\n",
    "            sheet = cssutils.parseString(content, encoding='latin-1')\n",
    "            \n",
    "        \n",
    "        for rule in sheet.cssRules:\n",
    "            if rule.type == rule.STYLE_RULE:\n",
    "                declarations_to_remove = [decl for decl in rule.style if is_text_decoration_none_important(decl)]\n",
    "                for decl in declarations_to_remove:\n",
    "                    print(rule)\n",
    "                    rule.style.cssText = 'text-decoration: none'\n",
    "                    print(rule)\n",
    "        try:\n",
    "            modification = bytes(sheet.cssText.decode('utf-8'),'utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            modification = bytes(sheet.cssText.decode('latin-1'),'utf-8')\n",
    "        \n",
    "        output_archive_epub.writestr(file_list[x].filename, modification)\n",
    "        output_archive_zip.writestr(file_list[x].filename, modification)\n",
    "    \n",
    "    else:\n",
    "        #For the other file types, simply copy the original content:\n",
    "        output_archive_epub.writestr(file_list[x].filename, content)\n",
    "        output_archive_zip.writestr(file_list[x].filename, content)\n",
    "#         print(content)\n",
    "\n",
    "input_archive.close()\n",
    "output_archive_epub.close()\n",
    "output_archive_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36mverbose\u001b[39m: Ace 1.3.2, Node v18.15.0, Windows_NT 10.0.22631\n",
      "\u001b[36mverbose\u001b[39m: Options:\n",
      "\u001b[32minfo\u001b[39m:    Processing post_patch.epub\n",
      "\u001b[36mverbose\u001b[39m: Extracting EPUB\n",
      "\u001b[32minfo\u001b[39m:    Parsing EPUB\n",
      "\u001b[36mverbose\u001b[39m: at location 'C:\\Users\\kolts\\AppData\\Local\\Temp\\tmp-13364-3V5mGe34uwPJ'\n",
      "\u001b[32minfo\u001b[39m:    Analyzing accessibility metadata\n",
      "\u001b[32minfo\u001b[39m:    Checking package...\n",
      "\u001b[32minfo\u001b[39m:    - EPUB\\content.opf: 5 issues found\n",
      "\u001b[32minfo\u001b[39m:    Checking documents...\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Cover.xhtml\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/TitlePage.xhtml\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/TOC.xhtml\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Preface.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Cover.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Cover.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Contributors.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/TitlePage.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/TitlePage.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Introduction.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Preface.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Preface.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Part01.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/TOC.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/TOC.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter01.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Contributors.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Contributors.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter02.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Part01.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Part01.xhtml: 1 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter03.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter02.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter02.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter04.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Introduction.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Introduction.xhtml: 1 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Part02.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter01.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter01.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter05.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter03.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter03.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter06.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Part02.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Part02.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter07.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter04.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter04.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Part03.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter05.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter05.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter08.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter06.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter06.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter09.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Part03.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Part03.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter10.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter07.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter07.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Part04.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter08.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter08.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter11.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Part04.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Part04.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter12.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter10.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter10.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter13.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter09.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter09.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Chapter14.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter11.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter11.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Index.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter12.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter12.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Series.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter13.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter13.xhtml: 2 issues found\n",
      "\u001b[36mverbose\u001b[39m: - Processing Text/Copyright.xhtml\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Series.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Series.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Copyright.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Copyright.xhtml: No issues found\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Chapter14.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Chapter14.xhtml: 3 issues found\n",
      "\u001b[36mverbose\u001b[39m: Converting aXe results to ace for Text/Index.xhtml\n",
      "\u001b[32minfo\u001b[39m:    - Text/Index.xhtml: No issues found\n",
      "\u001b[32minfo\u001b[39m:    Consolidating results...\n",
      "\u001b[32minfo\u001b[39m:    Copying data\n",
      "\u001b[32minfo\u001b[39m:    Saving JSON report\n",
      "\u001b[32minfo\u001b[39m:    Saving HTML report\n",
      "\u001b[32minfo\u001b[39m:    Done.\n",
      "\u001b[32minfo\u001b[39m:    Closing logs.\n",
      "Wall time: 9.75 s\n"
     ]
    }
   ],
   "source": [
    "!ace --verbose --force --subdir --outdir results post_patch.epub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
